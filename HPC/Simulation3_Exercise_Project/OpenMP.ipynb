{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 7: Shared Memory Parallel Programming with OpenMP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some basics</b>\n",
    "* Shared memory system => All the cores can access all the memory locations\n",
    "* OpenMP can be defined as multiprocessing, a directives-based shared memory API\n",
    "* an instance of a program running on a processor is called a thread (vs. a process in MPI)\n",
    "* In Python, Processor means a physical processor (separate memory space) and threading (same memory space) means a virtual entity (a small chunk inside a processor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After #pragma omp, i.e., the parallel directive, the block of code will be\n",
    "run inside all threads.\n",
    "* Clause num_threads can specify num of threads. \n",
    "* The team of threads, the master (original) and thread_count-1 (slaves) will call the code, after all the threads are finished, slave threads will be terminated and master thread continues.\n",
    "* omp_get_thread_num => rank or id of a thread\n",
    "* omp_get_num_threads => total number of threads\n",
    "* Compile on stromboli: <b> gcc -g -Wall -fopenmp -o omp_hello omp_hello.c </b> \n",
    "* Submit on strombolie : submit_script.sh\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "./omp_hello 4 > output\n",
    "\n",
    "```\n",
    "* parallel directive will be just ignored if OpenMP is not supported by the compiler. However to avoid error from include section, we can use\n",
    "\n",
    "```c\n",
    "#ifdef _OPENMP\n",
    "#include <omp.h>\n",
    "#endif\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 False Sharing and Padding\n",
    "\n",
    "* Symmetric Multi-Processor (SMP): a shared address space with equal time access for each processor; OS treats every processor the same way.\n",
    "* Non-Uniform Memory Access multiprocessor (NUMA): Different memory regions have different access costs. (near and far memory)\n",
    "\n",
    "<u> Any multiprocessor CPU with a cache is a NUMA system.</u>\n",
    "\n",
    "* OpenMP is a multi-threading, shared address model; threads communicate by sharing variables\n",
    "* OS scheduler decides when to run which threads ... interleaved fairness\n",
    "* To avoid race conditions, synchronization can be used but it is expensive.\n",
    "* Change how data is accessed to minimize the need for synchronization\n",
    "* <i> SPMD program can be a good solution. </i>\n",
    "\n",
    "<b><i> If independent data elements happen to sit on the same cache line, each update will cause the cahce lines to slosh back and forth between threads, this is called false sharing. <br/>\n",
    "Pad arrays so elements you use are on distinct cache lines. <br/>\n",
    "Padding array requires deep knowledge of the cache architecture, systems have different sized cache lines => software performance may fall apart\n",
    "<i><b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Scope of variables and the reduction clause\n",
    "* Shared scope (global)\n",
    "* Private scope (local)\n",
    "\n",
    "Three ways we can make an operation thread safe\n",
    "\n",
    "* Critical: stops all the other thread except one, so the operation can be safely done and stored.\n",
    "* Atomic: A special type only when we have the form like x \\<op\\>= \\<expression>, x++, ++x, --x, x--. This can be faster than an ordinary ciritical section, this is made to exploit special hardware. \n",
    "* Reduction: OpenMP creates private variables for all threads and at the end it runs the mentioned operation. Method overloading is not available. \n",
    "\n",
    "```c\n",
    "// just before the operation\n",
    "#pragma omp cirtical\n",
    "#pragma omp atomic\n",
    "\n",
    "// before the code block\n",
    "#pragma omp parallel_num_threads(thread_count) reduction(+: result)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between MPI and OpenMP\n",
    "<table>\n",
    "<tr>\n",
    "    <td>MPI</td>\n",
    "    <td>OpenMP</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Distributed memory</td>\n",
    "    <td>Shared memory</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beea94e32aef35cef7dc49f34ae9a2e1d5018c36e2514478a8c67a1d7e1945a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
